{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Football-QLearn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGnNN28yIlFedYGY/stThI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/volodymyr-d/Holacracy-Constitution-4.1-RUSSIAN/blob/master/Football_QLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj_6KCqUJz3q"
      },
      "source": [
        "# Install:\n",
        "# Kaggle environments.\n",
        "!git clone https://github.com/Kaggle/kaggle-environments.git\n",
        "!cd kaggle-environments && pip install .\n",
        "\n",
        "# GFootball environment.\n",
        "!apt-get update -y\n",
        "!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n",
        "\n",
        "# Make sure that the Branch in git clone and in wget call matches !!\n",
        "!git clone -b v2.7 https://github.com/google-research/football.git\n",
        "!mkdir -p football/third_party/gfootball_engine/lib\n",
        "\n",
        "!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n",
        "!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . \n",
        "\n",
        "# Install Gym\n",
        "!pip install gym\n",
        "\n",
        "!pip install tqm\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQL0R2CjJ4_D",
        "outputId": "58a008d0-5870-448d-a575-77b92e3e13b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gfootball\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import gym\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "def plot(data, window=100):\n",
        "    sns.lineplot(\n",
        "        data=data.rolling(window=window).mean()[window-1::window]\n",
        "    )\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size=1000000):\n",
        "        self.memory = deque(maxlen=size)\n",
        "        \n",
        "    def remember(self, s_t, a_t, r_t, s_t_next, d_t):\n",
        "        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))\n",
        "        \n",
        "    def sample(self,batch_size):\n",
        "        batch_size = min(batch_size, len(self.memory))\n",
        "        return random.sample(self.memory,batch_size)\n",
        "\n",
        "\n",
        "class SplitLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        split0,split1,split2,split3 = tf.split(inputs,4,3)\n",
        "        return [split0,split1,split2,split3]\n",
        "\n",
        "class ConvNN:\n",
        "\n",
        "    def __init__(self,state_shape,num_actions):\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def _model_architecture(self):\n",
        "    \n",
        "        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=3)\n",
        "        frames_split = SplitLayer()(frames_input)\n",
        "        conv_branches = []\n",
        "        for f,frame in enumerate(frames_split):\n",
        "            conv_branches.append(self._build_conv_branch(frame,f))\n",
        "\n",
        "        concat = tf.keras.layers.concatenate(conv_branches)\n",
        "        \n",
        "        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', \n",
        "                                     activation='relu')(concat)\n",
        "        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', \n",
        "                                     activation='relu')(fc0)\n",
        "        fc2 = tf.keras.layers.Dense(units=512, name='fc2', \n",
        "                                     activation='relu')(fc1)\n",
        "        fc3 = tf.keras.layers.Dense(units=256, name='fc3', \n",
        "                                     activation='relu')(fc2)\n",
        "        fc4 = tf.keras.layers.Dense(units=64, name='fc4', \n",
        "                                     activation='relu')(fc3)\n",
        "        \n",
        "        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',\n",
        "                                               activation='relu')(fc4)\n",
        "        return frames_input,action_output\n",
        "        \n",
        "    @staticmethod\n",
        "    def _build_conv_branch(frame,number):\n",
        "        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),\n",
        "                                        name='conv1_frame'+str(number), padding='same',\n",
        "                                        activation='relu')(frame)\n",
        "        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp1_frame\"+str(number))(conv1)\n",
        "        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),\n",
        "                                        name='conv2_frame'+str(number),padding='same',\n",
        "                                        activation='relu')(mp1)\n",
        "        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp2_frame\"+str(number))(conv2)\n",
        "        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n",
        "                                        name='conv3_frame'+str(number),padding='same',\n",
        "                                        activation='relu')(mp2)\n",
        "        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"mp3_frame\"+str(number))(conv3)\n",
        "        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),\n",
        "                                        name='conv4_frame'+str(number), padding='same',\n",
        "                                        activation='relu')(mp3)\n",
        "        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=\"mp4_frame\"+str(number))(conv4)\n",
        "\n",
        "        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)\n",
        "\n",
        "        return flatten\n",
        "        \n",
        "    def build(self):\n",
        "        frames_input,action_output = self._model_architecture()\n",
        "        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])\n",
        "        return model\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_shape, num_actions,alpha, gamma, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1):\n",
        "        self.epsilon_i = epsilon_i\n",
        "        self.epsilon_f = epsilon_f\n",
        "        self.n_epsilon = n_epsilon\n",
        "        self.epsilon = epsilon_i\n",
        "        self.gamma = gamma\n",
        "        self.state_shape = state_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.optimizer = tf.keras.optimizers.Adam(alpha) \n",
        "\n",
        "        self.Q = ConvNN(state_shape,num_actions).build()\n",
        "        self.Q_ = ConvNN(state_shape,num_actions).build()\n",
        "        \n",
        "    def synchronize(self):\n",
        "        self.Q_.set_weights(self.Q.get_weights())\n",
        "\n",
        "    def act(self, s_t):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions,size=3)\n",
        "        return np.argmax(self.Q(s_t), axis=1)\n",
        "    \n",
        "    def decay_epsilon(self, n):\n",
        "        self.epsilon = max(\n",
        "            self.epsilon_f, \n",
        "            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))\n",
        "\n",
        "    def update(self, s_t, a_t, r_t, s_t_next, d_t):\n",
        "        with tf.GradientTape() as tape:\n",
        "            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1))\n",
        "            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float32), axis=1)\n",
        "            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)\n",
        "        grads = tape.gradient(loss, self.Q.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))\n",
        "\n",
        "class VectorizedEnvWrapper(gym.Wrapper):\n",
        "    def __init__(self, make_env, num_envs=1):\n",
        "        super().__init__(make_env())\n",
        "        self.num_envs = num_envs\n",
        "        self.envs = [make_env() for env_index in range(num_envs)]\n",
        "    \n",
        "    def reset(self):\n",
        "        return np.asarray([env.reset() for env in self.envs])\n",
        "    \n",
        "    def reset_at(self, env_index):\n",
        "        return self.envs[env_index].reset()\n",
        "    \n",
        "    def step(self, actions):\n",
        "        next_states, rewards, dones, infos = [], [], [], []\n",
        "        for env, action in zip(self.envs, actions):\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            infos.append(info)\n",
        "        return np.asarray(next_states), np.asarray(rewards), \\\n",
        "            np.asarray(dones), np.asarray(infos)\n",
        "\n",
        "class NormalizationWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "    \n",
        "    def observation(self, obs):\n",
        "        return obs/255\n",
        "\n",
        "train_agent = Agent((72,96,4),19,alpha=0.1,gamma=0.95)\n",
        "train_agent.Q.save_weights(\"QWeights\")\n",
        "train_agent.Q_.save_weights(\"Q_Weights\")\n",
        "smm_env = VectorizedEnvWrapper(lambda: NormalizationWrapper(gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")), 3)\n",
        "train_buffer = ReplayBuffer()      \n",
        "\n",
        "\n",
        "def train(env=smm_env,T=3001,batch_size=3,sync_every=10,agent=train_agent,buffer=train_buffer):\n",
        "    \n",
        "    agent.Q.load_weights(\"QWeights\")\n",
        "    agent.Q_.load_weights(\"Q_Weights\")\n",
        "    rewards = []\n",
        "    episode_rewards = 0\n",
        "    s_t = env.reset()\n",
        "    start = time.time()\n",
        "    for t in range(T):\n",
        "        if t%sync_every == 0:\n",
        "            agent.synchronize()\n",
        "        a_t = agent.act(s_t)\n",
        "        s_t_next, r_t, d_t, info = env.step(a_t)\n",
        "        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)\n",
        "        for batch in buffer.sample(batch_size):\n",
        "            agent.update(*batch)\n",
        "        agent.decay_epsilon(t/T)\n",
        "        episode_rewards += r_t\n",
        "                  \n",
        "        for i in range(env.num_envs): \n",
        "            if d_t[i]:\n",
        "                rewards.append(episode_rewards[i])   \n",
        "                episode_rewards[i] = 0\n",
        "                s_t[i] = env.reset_at(i)\n",
        "\n",
        "       # if t % 100 == 0 and False:\n",
        "       #   print(t)\n",
        "       #   done = time.time()\n",
        "       #   print(done - start)\n",
        "       #   start = done\n",
        "       #   print(rewards)\n",
        "        \n",
        "    agent.epsilon_i = 1.0\n",
        "    agent.epsilon_f = 0.01\n",
        "    agent.n_epsilon = 0.1\n",
        "    agent.epsilon = agent.epsilon_i\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "print(\"Start\")\n",
        "#rewards = train()\n",
        "#plot(pd.DataFrame(rewards), window=10)\n",
        "train_agent.Q.save_weights(\"QWeights\") "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OR3hPQW8wer"
      },
      "source": [
        "import gfootball\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import gym\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "  \n",
        "  \n",
        "def show_state(env, step=0):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='ipython'))\n",
        "    plt.title(\"%s. Step: %d\" % (env._spec.id,step))\n",
        "    \n",
        "    plt.pause(0.001)  # pause for plots to update\n",
        "\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "  \n",
        "  \n",
        "env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\n",
        "next_state, reward, done, info = env.step(5)\n",
        "env.render(mode='ipython')\n",
        "print(next_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMs15fyBCku2"
      },
      "source": [
        "from kaggle_environments import make\n",
        "from tqdm import tqdm\n",
        "import gym\n",
        "from typing import Union, Callable, List, Tuple, Iterable, Any, Dict\n",
        "\n",
        "env = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"5_vs_5\", \"running_in_notebook\": True})\n",
        "\n",
        "env.reset()\n",
        "steps = 0\n",
        "for x in (range(50)):\n",
        "  a1 = 5\n",
        "  a2 = 0\n",
        "  obs, rew = env.step([[a1], [a2]])\n",
        "  print(obs['reward'])\n",
        "  print(rew)\n",
        "  print(obs['observation']['players_raw'][0]['ball'])\n",
        "  print(obs['observation']['players_raw'][0]['ball_owned_player'])\n",
        "  print(obs['observation']['players_raw'][0]['ball_owned_team'])\n",
        "  print(obs['observation']['players_raw'][0]['right_team'])\n",
        "  print(obs['observation']['players_raw'][0]['left_team'])\n",
        "  break\n",
        "  if env.done:\n",
        "    break\n",
        "env.render(mode=\"human\", width=800, height=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZQIHk3DKjOh",
        "outputId": "966a859f-4058-4137-c80e-2dc713c70cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from kaggle_environments import make\n",
        "from tqdm import tqdm\n",
        "import gym\n",
        "from typing import Union, Callable, List, Tuple, Iterable, Any, Dict\n",
        "from google.colab import output\n",
        "\n",
        "\n",
        "def convert(point):\n",
        "    return int(point[0] * 100) + 98, int(point[1] * 100) + 41\n",
        "\n",
        "def get_field(obs):\n",
        "    ball_owned_player = obs['observation']['players_raw'][0]['ball_owned_player']\n",
        "    ball_owned_team = obs['observation']['players_raw'][0]['ball_owned_team']\n",
        "    ball = obs['observation']['players_raw'][0]['ball']\n",
        "    right_team = obs['observation']['players_raw'][0]['right_team']\n",
        "    left_team = obs['observation']['players_raw'][0]['left_team']\n",
        "    field = np.zeros((200, 82))\n",
        "\n",
        "    for i, player in enumerate(left_team):\n",
        "      mark = 1\n",
        "      if ball_owned_player == i:\n",
        "        mark = 5\n",
        "      point = convert(player)\n",
        "      field[point[0]][point[1]] = mark\n",
        "\n",
        "    for i, player in enumerate(right_team):\n",
        "      mark = 2\n",
        "      point = convert(player)\n",
        "      field[point[0]][point[1]] = mark\n",
        "\n",
        "    point = convert((ball[0], ball[1]))\n",
        "    field[point[0]][point[1]] = 3 \n",
        "    return field\n",
        "\n",
        "def render(field):\n",
        "    s = 3\n",
        "    im = Image.new('RGB', (200*s, 82*s), (128, 128, 128))\n",
        "    draw = ImageDraw.Draw(im)     \n",
        "    for x in range(200):\n",
        "      for y in range(82):\n",
        "        color = (128, 128, 128)\n",
        "        if field[x][y] == 1:\n",
        "          color = (255, 0, 0)\n",
        "        if field[x][y] == 2:\n",
        "          color = (255, 255, 0)\n",
        "        if field[x][y] == 3:\n",
        "          color = (0, 255, 255)\n",
        "        if field[x][y] == 5:\n",
        "          color = (0, 255, 0)                 \n",
        "        draw.rectangle(((x-1)*s, (y-1)*s, (x+1)*s, (y+1)*s), fill=color)  \n",
        "    \n",
        "    output.clear(output_tags='some_outputs')\n",
        "    with output.use_tags('some_outputs'):\n",
        "      display(im)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "steps = 0\n",
        "for x in (range(5000)):\n",
        "  a1 = 5\n",
        "  a2 = 0\n",
        "  obs, rew = env.step([[a1], [a2]])\n",
        "  field = get_field(obs)\n",
        "  render(field)\n",
        "  if env.done:\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-caba53ecaa46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_environments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_environments'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}